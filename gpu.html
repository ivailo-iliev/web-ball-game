<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>WebGPU video → compute(f16) → display (mobile-optimized)</title>
  <style>
    html, body { margin:0; height:100%; background:#000; color:#fff; font:14px system-ui, sans-serif; }
    #wrap { display:grid; place-items:center; gap:12px; height:100%; }
    #gfx { width: min(100vw, 100vh * 9/16); height: auto; image-rendering: pixelated; background:#000; }
    button { padding:10px 16px; font-weight:600; }
    .row { display:flex; gap:12px; align-items:center; }
  </style>
</head>
<body>
  <div id="wrap">
    <canvas id="gfx"></canvas>
    <div class="row">
      <button id="start">Start</button>
      <span id="info"></span>
    </div>
  </div>

  <script type="module">
    const $ = s => document.querySelector(s);
    const info = $('#info');
    const start = $('#start');
    const canvas = $('#gfx');

    // WGSL — compute (f16), 1:1 loads, red checker overlay
    const computeWGSL = /* wgsl */`
      enable f16;

      struct Params {
        size : vec2<u32>,
        _pad : vec2<u32>,
      };

      @group(0) @binding(0) var src : texture_2d<f32>;
      @group(0) @binding(1) var dst : texture_storage_2d<rgba8unorm, write>;
      @group(0) @binding(2) var<uniform> params : Params;

      @compute @workgroup_size(8, 8, 1)
      fn main(@builtin(global_invocation_id) gid : vec3<u32>) {
        if (gid.x >= params.size.x || gid.y >= params.size.y) { return; }

        // exact pixel fetch (no sampler/filtering)
        let c = textureLoad(src, vec2<i32>(gid.xy), 0);

        // prove f16 path is active
        var c16 = vec4<f16>(c);

        // cheap, coherent branch: paint a red checker
        if (((gid.x ^ gid.y) & 8u) == 0u) {
          c16 = vec4<f16>(1.0h, 0.0h, 0.0h, 1.0h);
        }

        textureStore(dst, vec2<i32>(gid.xy), vec4<f32>(c16));
      }
    `;

    // WGSL — fullscreen blit (render)
    const blitWGSL = /* wgsl */`
      struct VSOut {
        @builtin(position) pos : vec4<f32>,
        @location(0) uv : vec2<f32>,
      };

      @vertex
      fn vs(@builtin(vertex_index) vid : u32) -> VSOut {
        var p = array<vec2<f32>, 3>(
          vec2<f32>(-1.0, -1.0), vec2<f32>(-1.0,  3.0), vec2<f32>( 3.0, -1.0));
        var t = array<vec2<f32>, 3>(
          vec2<f32>( 0.0,  0.0), vec2<f32>( 0.0,  2.0), vec2<f32>( 2.0,  0.0));
        var o: VSOut;
        o.pos = vec4<f32>(p[vid], 0.0, 1.0);
        o.uv  = t[vid];
        return o;
      }

      @group(0) @binding(0) var s   : sampler;
      @group(0) @binding(1) var tex : texture_2d<f32>;

      @fragment
      fn fs(i : VSOut) -> @location(0) vec4<f32> {
        return textureSampleLevel(tex, s, i.uv, 0.0);
      }
    `;

    start.addEventListener('click', async () => {
      start.disabled = true;
      try {
        // 1) Camera → WebCodecs VideoFrame (no <video> element)
        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' }, audio: false });
        const track = stream.getVideoTracks()[0];
        const processor = new MediaStreamTrackProcessor({ track });
        const reader = processor.readable.getReader();

        // 2) WebGPU (require f16; no fallbacks)
        if (!('gpu' in navigator)) throw new Error('WebGPU not available');
        const adapter = await navigator.gpu.requestAdapter({ powerPreference: 'high-performance' });
        if (!adapter) throw new Error('No GPU adapter');
        const device = await adapter.requestDevice({ requiredFeatures: ['shader-f16'] });

        const ctx = canvas.getContext('webgpu');
        const format = navigator.gpu.getPreferredCanvasFormat();
        ctx.configure({ device, format, alphaMode: 'opaque' });

        // 3) Buffers (textures are created lazily from first VideoFrame)
        let srcTex, outTex;
        let width = 0, height = 0;
        let xGroups = 0, yGroups = 0;
        let computeBG, blitBundle;
        const WG = 8;

        const paramsBuf = device.createBuffer({
          size: 16, // vec2<u32> + padding
          usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
        });

        // 4) Pipelines (auto layouts)
        const computeMod = device.createShaderModule({ code: computeWGSL });
        const computePipeline = device.createComputePipeline({
          layout: 'auto',
          compute: { module: computeMod, entryPoint: 'main' }
        });

        const blitMod = device.createShaderModule({ code: blitWGSL });
        const blitPipeline = device.createRenderPipeline({
          layout: 'auto',
          vertex:   { module: blitMod, entryPoint: 'vs' },
          fragment: { module: blitMod, entryPoint: 'fs', targets: [{ format }] },
          primitive: { topology: 'triangle-list' },
        });
        // Helper to (re)create textures/bind groups/bundle from a VideoFrame's coded size
        async function ensureTextures(frame) {
          const w = frame.codedWidth, h = frame.codedHeight;
          if (srcTex && w === width && h === height) return;

          width = w; height = h;
          canvas.width = frame.displayWidth;
          canvas.height = frame.displayHeight;

          srcTex?.destroy?.();
          outTex?.destroy?.();

          srcTex = device.createTexture({
            size: { width, height },
            format: 'rgba8unorm',
            usage:
              GPUTextureUsage.TEXTURE_BINDING |
              GPUTextureUsage.COPY_DST |
              GPUTextureUsage.RENDER_ATTACHMENT
          });

          outTex = device.createTexture({
            size: { width, height },
            format: 'rgba8unorm',
            usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING
          });

          device.queue.writeBuffer(paramsBuf, 0, new Uint32Array([width, height, 0, 0]));
          xGroups = Math.ceil(width / WG);
          yGroups = Math.ceil(height / WG);

          computeBG = device.createBindGroup({
            layout: computePipeline.getBindGroupLayout(0),
            entries: [
              { binding: 0, resource: srcTex.createView() },
              { binding: 1, resource: outTex.createView() },
              { binding: 2, resource: { buffer: paramsBuf } },
            ]
          });

          const blitBG = device.createBindGroup({
            layout: blitPipeline.getBindGroupLayout(0),
            entries: [
              { binding: 0, resource: device.createSampler({ magFilter: 'nearest', minFilter: 'nearest' }) },
              { binding: 1, resource: outTex.createView() },
            ]
          });

          const bundleEnc = device.createRenderBundleEncoder({ colorFormats: [format] });
          bundleEnc.setPipeline(blitPipeline);
          bundleEnc.setBindGroup(0, blitBG);
          bundleEnc.draw(3);
          blitBundle = bundleEnc.finish();

          info.textContent = `Running ${width}×${height}, f16 compute + bundle blit (VideoFrame).`;
        }
        // 5) Decode-paced pump: read VideoFrames and push them through CEITT → compute → blit
        (async function pump() {
          for (;;) {
            const { value: frame, done } = await reader.read();
            if (done || !frame) break;
            try {
              await ensureTextures(frame);

              device.queue.copyExternalImageToTexture(
                { source: frame  , flipY:true  },
                { texture: srcTex, colorSpace: 'srgb' },
                { width, height }
              );

              const enc = device.createCommandEncoder();
              {
                const pass = enc.beginComputePass();
                pass.setPipeline(computePipeline);
                pass.setBindGroup(0, computeBG);
                pass.dispatchWorkgroups(xGroups, yGroups);
                pass.end();
              }
              {
                const colorView = ctx.getCurrentTexture().createView();
                const pass = enc.beginRenderPass({
                  colorAttachments: [{ view: colorView, loadOp: 'load', storeOp: 'store' }]
                });
                pass.executeBundles([blitBundle]);
                pass.end();
              }
              device.queue.submit([enc.finish()]);
            } finally {
              frame.close();
            }
          }
        })();

      } catch (err) {
        info.textContent = (err && err.message) ? err.message : String(err);
        start.disabled = false;
        console.error(err);
      }
    });
  </script>
</body>
</html>
